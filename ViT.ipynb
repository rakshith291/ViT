{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ViT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPveiqGhZvrKMxKSd7CPfaI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakshith291/ViT/blob/main/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMWuLIyQ5Nye"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import utils\n",
        "import math\n",
        "import numpy as np \n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8KociIABWH2"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'att':self.att,\n",
        "            'ffn':self.ffn,\n",
        "            'layernorm1':self.layernorm1,\n",
        "            'layernorm2':self.layernorm2,\n",
        "            'dropout1':self.dropout1,\n",
        "            'dropout2':self.dropout2\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output,attn_scores = self.att(inputs, inputs,return_attention_scores = True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output),attn_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCLmOsqmIR2M"
      },
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "  def __init__(self,num_pathces,proj_dim):\n",
        "    super(TokenEmbedding,self).__init__()\n",
        "    self.num_patches = num_patches\n",
        "    self.dense_projection = layers.Dense(proj_dim)  # linear projection\n",
        "    self.pos_emb = layers.Embedding(input_dim=num_patches, output_dim=proj_dim)\n",
        "    \n",
        "    \n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({\n",
        "        'num_patches':self.num_patches,\n",
        "        'dense_projection':self.dense_projection,\n",
        "        'pos_emb':self.pos_emb\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def call(self,inputs):\n",
        "    positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "    embeddings = self.dense_projection(inputs)\n",
        "    positions = self.pos_emb(positions)\n",
        "    return embeddings + positions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1o0dIwpQZeH"
      },
      "source": [
        "class Patch_creation(layers.Layer) :\n",
        "  def __init__(self,num_patches_h,num_patches_w,patch_h,patch_w,embed_dim):\n",
        "    super(Patch_creation,self).__init__()\n",
        "    self.num_patches_h = num_patches_h\n",
        "    self.num_patches_w = num_patches_w\n",
        "    self.patch_h = patch_h\n",
        "    self.patch_w = patch_w\n",
        "    self.num_patches = self.num_patches_h*self.num_patches_w\n",
        "    self.embed_dim = embed_dim\n",
        "    self.flatten = layers.Flatten()\n",
        "    \n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({\n",
        "        'num_patches_h':self.num_patches_h,\n",
        "        'num_patches_w':self.num_patches_w,\n",
        "        'patch_h': self.patch_h,\n",
        "        'patch_w': self.patch_w,\n",
        "        'num_patches': self.num_patches_h*self.num_patches_w,\n",
        "        'flatten': self.flatten,\n",
        "        'embed_dim':self.embed_dim\n",
        "    })\n",
        "    return config \n",
        "\n",
        "  @tf.function\n",
        "  def call(self,imgs):\n",
        "    self.patches = tf.Variable([[[0]*(self.patch_h*self.patch_w*3) for i in range(self.num_patches)]],shape=[None,self.num_patches,(self.patch_h*self.patch_w*3)],dtype=tf.float32,validate_shape=False,trainable=False)\n",
        "    if imgs.shape[0] is not None :\n",
        "      self.patches = tf.Variable(tf.zeros(shape=(imgs.shape[0],self.num_patches,self.patch_h*self.patch_w*3)),shape=tf.TensorShape([imgs.shape[0],self.num_patches,self.patch_h*self.patch_w*3]),dtype='float32',validate_shape=False,trainable=False)\n",
        "    for n_h in range(self.num_patches_h):\n",
        "      for n_w in range(self.num_patches_w) :\n",
        "        patch = imgs[:,n_h*self.patch_h:(n_h+1)*self.patch_h,(n_w)*self.patch_w:(n_w+1)*self.patch_w,:]\n",
        "        patch = self.flatten(patch)\n",
        "        patch = tf.cast(patch,tf.float32)\n",
        "        self.patches[:,n_h*self.num_patches_w+n_w,:].assign(patch)\n",
        "        \n",
        "    return tf.convert_to_tensor(self.patches)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZmW5eEql6gF"
      },
      "source": [
        "embed_dim = 64\n",
        "num_patches_h = 8\n",
        "num_patches_w = 8\n",
        "patch_h = 8\n",
        "patch_w = 8\n",
        "num_patches = 64\n",
        "num_heads = 12 \n",
        "ffn_dim = 128\n",
        "image_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVZk4Ppiy1XK"
      },
      "source": [
        "data_augumentation = tf.keras.Sequential(\n",
        "    [\n",
        "     layers.experimental.preprocessing.Rescaling(scale=1./255),\n",
        "     layers.experimental.preprocessing.Resizing(image_size, image_size),\n",
        "     layers.experimental.preprocessing.RandomFlip(\"horizontal\")\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQb7z5Mj19Bx"
      },
      "source": [
        "inputs = layers.Input(shape=(32,32,3))\n",
        "augumented_data = data_augumentation(inputs)\n",
        "create_patches = Patch_creation(num_patches_h,num_patches_w,patch_h,patch_w,embed_dim)\n",
        "patches = create_patches(augumented_data)\n",
        "tokens = TokenEmbedding(num_patches,embed_dim)\n",
        "embeddings = tokens(patches)\n",
        "\n",
        "transformer_block = TransformerBlock(embed_dim,num_heads, ffn_dim)\n",
        "for  i in range(12):\n",
        "  embeddings ,attn_scores = transformer_block(embeddings)\n",
        "\n",
        "\n",
        "x = layers.GlobalAveragePooling1D()(embeddings)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFB1eCrZo0d3"
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "class myCallBack:\n",
        "    def custom_callbacks(self):\n",
        "      my_callbacks = [\n",
        "                      EarlyStopping(monitor='val_loss',patience=10,min_delta=0.01),\n",
        "                      ModelCheckpoint('model_1.h5',monitor='val_accuracy',save_best_only=True)\n",
        "                      \n",
        "      ]\n",
        "      return my_callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4I3mDcqotaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549a4716-4f76-4cc4-90ba-f4e73549c743"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "y_train = utils.to_categorical(y_train,10) # CIFAR-10 \n",
        "y_test = utils.to_categorical(y_test,10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvRlCbUhqGGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa058831-5421-4860-dc5d-dc1cf1044d66"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential (Sequential)         (None, 64, 64, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "patch_creation (Patch_creation) (None, 64, 192)      0           sequential[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "token_embedding (TokenEmbedding (None, 64, 64)       16448       patch_creation[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "transformer_block (TransformerB ((None, 64, 64), (No 215808      token_embedding[0][0]            \n",
            "                                                                 transformer_block[0][0]          \n",
            "                                                                 transformer_block[1][0]          \n",
            "                                                                 transformer_block[2][0]          \n",
            "                                                                 transformer_block[3][0]          \n",
            "                                                                 transformer_block[4][0]          \n",
            "                                                                 transformer_block[5][0]          \n",
            "                                                                 transformer_block[6][0]          \n",
            "                                                                 transformer_block[7][0]          \n",
            "                                                                 transformer_block[8][0]          \n",
            "                                                                 transformer_block[9][0]          \n",
            "                                                                 transformer_block[10][0]         \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 64)           0           transformer_block[11][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 64)           0           global_average_pooling1d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           650         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 232,906\n",
            "Trainable params: 232,906\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGRyvu_9o6GI",
        "outputId": "915f314a-9db1-4666-d528-55d84fa0c09e"
      },
      "source": [
        "lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    0.001, 58800)\n",
        "optimizer = tf.keras.optimizers.SGD(\n",
        "    learning_rate= lr_decayed_fn, momentum=0.9, nesterov=False, name=\"SGD\"\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer,loss=tf.keras.losses.CategoricalCrossentropy(),metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks_custom = myCallBack()\n",
        "callbacks = callbacks_custom.custom_callbacks()\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=256, epochs=300, validation_data=(x_test, y_test),callbacks=callbacks\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "196/196 [==============================] - 94s 466ms/step - loss: 2.5517 - accuracy: 0.1270 - val_loss: 2.0584 - val_accuracy: 0.2073\n",
            "Epoch 2/300\n",
            "125/196 [==================>...........] - ETA: 30s - loss: 2.0914 - accuracy: 0.2127"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}